{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Satoshi Poster Binary Extractor - Validation and Tuning\n",
        "\n",
        "This notebook provides tools to validate the binary extraction pipeline and tune its parameters to achieve optimal results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Hello, world!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import yaml\n",
        "\n",
        "# Add the project root to the path so we can import our modules\n",
        "project_root = Path(os.getcwd()).parent.parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import our modules\n",
        "from binary_extractor.extractor.pipeline import Pipeline\n",
        "from binary_extractor.extractor.grid import GridDetector\n",
        "from binary_extractor.extractor.classify import CellClassifier\n",
        "from binary_extractor.extractor.utils import load_config, setup_logger\n",
        "\n",
        "# Set up plotting\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Configure warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load reference data\n",
        "reference_digits_path = Path(project_root) / \"recognized_digits.csv\"\n",
        "reference_overlay_path = Path(project_root) / \"overlay_unknown_cells.csv\"\n",
        "\n",
        "# Check if reference files exist\n",
        "if not reference_digits_path.exists():\n",
        "    print(f\"Warning: Reference digits file not found at {reference_digits_path}\")\n",
        "    reference_digits = None\n",
        "else:\n",
        "    reference_digits = pd.read_csv(reference_digits_path)\n",
        "    print(f\"Loaded reference digits: {len(reference_digits)} entries\")\n",
        "    display(reference_digits.head())\n",
        "    \n",
        "if not reference_overlay_path.exists():\n",
        "    print(f\"Warning: Reference overlay file not found at {reference_overlay_path}\")\n",
        "    reference_overlay = None\n",
        "else:\n",
        "    reference_overlay = pd.read_csv(reference_overlay_path)\n",
        "    print(f\"Loaded reference overlay data: {len(reference_overlay)} entries\")\n",
        "    display(reference_overlay.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load the most recent pipeline output\n",
        "def load_pipeline_output(base_dir=None):\n",
        "    if base_dir is None:\n",
        "        # Check for output directories in order of preference\n",
        "        possible_dirs = [\n",
        "            Path(project_root) / \"binary_extractor\" / \"output\",\n",
        "            Path(project_root) / \"binary_extractor\" / \"output2\",\n",
        "            Path(project_root) / \"output3\",\n",
        "        ]\n",
        "        \n",
        "        # Find the most recent directory with output files\n",
        "        for dir_path in possible_dirs:\n",
        "            if dir_path.exists():\n",
        "                # Look for recognized_digits.csv in this directory\n",
        "                digits_file = dir_path / \"recognized_digits.csv\"\n",
        "                overlay_file = dir_path / \"overlay_unknown_cells.csv\"\n",
        "                \n",
        "                if digits_file.exists() or overlay_file.exists():\n",
        "                    base_dir = dir_path\n",
        "                    break\n",
        "    \n",
        "    if base_dir is None:\n",
        "        print(\"No output directory found with pipeline results\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Load digits file if it exists\n",
        "    digits_file = base_dir / \"recognized_digits.csv\"\n",
        "    if digits_file.exists():\n",
        "        pipeline_digits = pd.read_csv(digits_file)\n",
        "        print(f\"Loaded pipeline digits from {digits_file}: {len(pipeline_digits)} entries\")\n",
        "        display(pipeline_digits.head())\n",
        "    else:\n",
        "        pipeline_digits = None\n",
        "        print(f\"No recognized digits file found at {digits_file}\")\n",
        "    \n",
        "    # Load overlay file if it exists\n",
        "    overlay_file = base_dir / \"overlay_unknown_cells.csv\"\n",
        "    if overlay_file.exists():\n",
        "        pipeline_overlay = pd.read_csv(overlay_file)\n",
        "        print(f\"Loaded pipeline overlay data from {overlay_file}: {len(pipeline_overlay)} entries\")\n",
        "        display(pipeline_overlay.head())\n",
        "    else:\n",
        "        pipeline_overlay = None\n",
        "        print(f\"No overlay data file found at {overlay_file}\")\n",
        "    \n",
        "    return base_dir, pipeline_digits, pipeline_overlay\n",
        "\n",
        "# Load the most recent pipeline output\n",
        "output_dir, pipeline_digits, pipeline_overlay = load_pipeline_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze distribution of 0s and 1s in both datasets\n",
        "def analyze_digit_distribution(df, title=\"Digit Distribution\"):\n",
        "    if df is None:\n",
        "        print(f\"Cannot analyze distribution for {title}: No data available\")\n",
        "        return None\n",
        "    \n",
        "    # Count 0s and 1s\n",
        "    digit_counts = df['digit'].value_counts().sort_index()\n",
        "    \n",
        "    # Calculate percentages\n",
        "    total = len(df)\n",
        "    percentages = digit_counts / total * 100\n",
        "    \n",
        "    # Create a DataFrame for display\n",
        "    distribution_df = pd.DataFrame({\n",
        "        'Count': digit_counts,\n",
        "        'Percentage': percentages\n",
        "    })\n",
        "    \n",
        "    # Display statistics\n",
        "    print(f\"\\n{title}:\")\n",
        "    print(f\"Total digits: {total}\")\n",
        "    print(f\"Distribution: {distribution_df.to_dict()}\")\n",
        "    \n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x=digit_counts.index, y=digit_counts.values)\n",
        "    \n",
        "    # Add count and percentage labels on bars\n",
        "    for i, (count, pct) in enumerate(zip(digit_counts, percentages)):\n",
        "        ax.text(i, count/2, f\"{count}\\n({pct:.1f}%)\", \n",
        "                ha='center', va='center', fontweight='bold')\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel('Digit')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return distribution_df\n",
        "\n",
        "# Analyze reference data distribution\n",
        "ref_distribution = analyze_digit_distribution(reference_digits, \"Reference Data Distribution\")\n",
        "\n",
        "# Analyze pipeline output distribution\n",
        "pipeline_distribution = analyze_digit_distribution(pipeline_digits, \"Pipeline Output Distribution\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare pipeline output with reference data\n",
        "def compare_datasets(pipeline_df, reference_df, title=\"Comparison with Reference Data\"):\n",
        "    if pipeline_df is None or reference_df is None:\n",
        "        print(f\"Cannot perform {title}: Missing data\")\n",
        "        return\n",
        "    \n",
        "    # Merge datasets on row and col\n",
        "    merged = pd.merge(\n",
        "        pipeline_df, \n",
        "        reference_df, \n",
        "        on=['row', 'col'], \n",
        "        how='outer',\n",
        "        suffixes=('_pipeline', '_reference')\n",
        "    )\n",
        "    \n",
        "    # Fill NaN values for better analysis\n",
        "    merged = merged.fillna({\n",
        "        'digit_pipeline': -1,  # -1 indicates missing in pipeline\n",
        "        'digit_reference': -1  # -1 indicates missing in reference\n",
        "    })\n",
        "    \n",
        "    # Convert digits to integers for comparison\n",
        "    merged['digit_pipeline'] = merged['digit_pipeline'].astype(int)\n",
        "    merged['digit_reference'] = merged['digit_reference'].astype(int)\n",
        "    \n",
        "    # Calculate match status\n",
        "    merged['match_status'] = 'Unknown'\n",
        "    \n",
        "    # Both have valid digits (0 or 1)\n",
        "    valid_mask = (merged['digit_pipeline'].isin([0, 1])) & (merged['digit_reference'].isin([0, 1]))\n",
        "    merged.loc[valid_mask & (merged['digit_pipeline'] == merged['digit_reference']), 'match_status'] = 'Match'\n",
        "    merged.loc[valid_mask & (merged['digit_pipeline'] != merged['digit_reference']), 'match_status'] = 'Mismatch'\n",
        "    \n",
        "    # One has a digit, other doesn't\n",
        "    merged.loc[(merged['digit_pipeline'].isin([0, 1])) & (merged['digit_reference'] == -1), 'match_status'] = 'Extra in Pipeline'\n",
        "    merged.loc[(merged['digit_pipeline'] == -1) & (merged['digit_reference'].isin([0, 1])), 'match_status'] = 'Missing in Pipeline'\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_reference = len(reference_df)\n",
        "    total_pipeline = len(pipeline_df)\n",
        "    \n",
        "    match_counts = merged['match_status'].value_counts()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    matches = match_counts.get('Match', 0)\n",
        "    mismatches = match_counts.get('Mismatch', 0)\n",
        "    extras = match_counts.get('Extra in Pipeline', 0)\n",
        "    missing = match_counts.get('Missing in Pipeline', 0)\n",
        "    \n",
        "    accuracy = matches / total_reference if total_reference > 0 else 0\n",
        "    precision = matches / (matches + mismatches + extras) if (matches + mismatches + extras) > 0 else 0\n",
        "    recall = matches / (matches + mismatches + missing) if (matches + mismatches + missing) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\n{title}:\")\n",
        "    print(f\"Total in reference: {total_reference}\")\n",
        "    print(f\"Total in pipeline: {total_pipeline}\")\n",
        "    print(f\"Matches: {matches} ({matches/total_reference*100:.1f}% of reference)\")\n",
        "    print(f\"Mismatches: {mismatches} ({mismatches/total_reference*100:.1f}% of reference)\")\n",
        "    print(f\"Extra in pipeline: {extras}\")\n",
        "    print(f\"Missing in pipeline: {missing} ({missing/total_reference*100:.1f}% of reference)\")\n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    \n",
        "    # Visualize match status\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x=match_counts.index, y=match_counts.values)\n",
        "    \n",
        "    # Add count labels on bars\n",
        "    for i, count in enumerate(match_counts.values):\n",
        "        ax.text(i, count/2, str(count), ha='center', va='center', fontweight='bold')\n",
        "    \n",
        "    plt.title(f\"{title} - Match Status\")\n",
        "    plt.xlabel('Status')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Return the merged dataframe for further analysis\n",
        "    return merged\n",
        "\n",
        "# Compare digits\n",
        "comparison_df = compare_datasets(pipeline_digits, reference_digits, \"Digit Recognition Comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare overlay detection\n",
        "def compare_overlay_detection(pipeline_df, reference_df, title=\"Overlay Detection Comparison\"):\n",
        "    if pipeline_df is None or reference_df is None:\n",
        "        print(f\"Cannot perform {title}: Missing data\")\n",
        "        return\n",
        "    \n",
        "    # Merge datasets on row and col\n",
        "    merged = pd.merge(\n",
        "        pipeline_df, \n",
        "        reference_df, \n",
        "        on=['row', 'col'], \n",
        "        how='outer',\n",
        "        suffixes=('_pipeline', '_reference')\n",
        "    )\n",
        "    \n",
        "    # Fill NaN values to indicate missing entries\n",
        "    merged = merged.fillna({\n",
        "        'row': -1,\n",
        "        'col': -1\n",
        "    })\n",
        "    \n",
        "    # Calculate match status\n",
        "    merged['match_status'] = 'Unknown'\n",
        "    \n",
        "    # Both have the cell\n",
        "    both_have = (merged['row'] != -1) & (merged['col'] != -1)\n",
        "    merged.loc[both_have, 'match_status'] = 'Match'\n",
        "    \n",
        "    # Only in pipeline\n",
        "    only_pipeline = (merged['row'] != -1) & (merged['col'] != -1) & pd.isna(merged['row_reference'])\n",
        "    merged.loc[only_pipeline, 'match_status'] = 'Extra in Pipeline'\n",
        "    \n",
        "    # Only in reference\n",
        "    only_reference = (merged['row'] != -1) & (merged['col'] != -1) & pd.isna(merged['row_pipeline'])\n",
        "    merged.loc[only_reference, 'match_status'] = 'Missing in Pipeline'\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_reference = len(reference_df)\n",
        "    total_pipeline = len(pipeline_df)\n",
        "    \n",
        "    match_counts = merged['match_status'].value_counts()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    matches = match_counts.get('Match', 0)\n",
        "    extras = match_counts.get('Extra in Pipeline', 0)\n",
        "    missing = match_counts.get('Missing in Pipeline', 0)\n",
        "    \n",
        "    precision = matches / total_pipeline if total_pipeline > 0 else 0\n",
        "    recall = matches / total_reference if total_reference > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\n{title}:\")\n",
        "    print(f\"Total in reference: {total_reference}\")\n",
        "    print(f\"Total in pipeline: {total_pipeline}\")\n",
        "    print(f\"Matches: {matches} ({matches/total_reference*100:.1f}% of reference)\")\n",
        "    print(f\"Extra in pipeline: {extras}\")\n",
        "    print(f\"Missing in pipeline: {missing} ({missing/total_reference*100:.1f}% of reference)\")\n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    \n",
        "    # Visualize match status\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x=match_counts.index, y=match_counts.values)\n",
        "    \n",
        "    # Add count labels on bars\n",
        "    for i, count in enumerate(match_counts.values):\n",
        "        ax.text(i, count/2, str(count), ha='center', va='center', fontweight='bold')\n",
        "    \n",
        "    plt.title(f\"{title} - Match Status\")\n",
        "    plt.xlabel('Status')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Return the merged dataframe for further analysis\n",
        "    return merged\n",
        "\n",
        "# Compare overlay detection\n",
        "overlay_comparison_df = compare_overlay_detection(pipeline_overlay, reference_overlay, \"Overlay Detection Comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize debug artifacts\n",
        "def load_and_display_image(file_path, title=None):\n",
        "    \"\"\"Load and display an image with a title\"\"\"\n",
        "    if not Path(file_path).exists():\n",
        "        print(f\"Image not found: {file_path}\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        img = cv2.imread(str(file_path))\n",
        "        if img is None:\n",
        "            print(f\"Failed to load image: {file_path}\")\n",
        "            return None\n",
        "        \n",
        "        # Convert BGR to RGB for display\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.imshow(img_rgb)\n",
        "        if title:\n",
        "            plt.title(title)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return img_rgb\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Find and display debug artifacts if they exist\n",
        "def display_debug_artifacts(output_dir):\n",
        "    if output_dir is None:\n",
        "        print(\"No output directory provided\")\n",
        "        return\n",
        "    \n",
        "    # List of common debug artifacts\n",
        "    debug_files = [\n",
        "        (\"grid_overlay.png\", \"Grid Detection Overlay\"),\n",
        "        (\"bw_mask.png\", \"Binary Mask\"),\n",
        "        (\"cells_color.png\", \"Classified Cells\"),\n",
        "        (\"cells_with_digits.png\", \"Cells with Detected Digits\"),\n",
        "        (\"overlay_mask.png\", \"Overlay Detection Mask\")\n",
        "    ]\n",
        "    \n",
        "    for filename, title in debug_files:\n",
        "        file_path = output_dir / filename\n",
        "        if file_path.exists():\n",
        "            print(f\"\\nDisplaying {title}:\")\n",
        "            load_and_display_image(file_path, title)\n",
        "        else:\n",
        "            print(f\"\\n{title} not found at {file_path}\")\n",
        "\n",
        "# Display debug artifacts if output directory was found\n",
        "if output_dir:\n",
        "    display_debug_artifacts(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameter tuning and grid search\n",
        "def load_config():\n",
        "    \"\"\"Load the current configuration\"\"\"\n",
        "    config_path = Path(project_root) / \"binary_extractor\" / \"cfg.yaml\"\n",
        "    if not config_path.exists():\n",
        "        print(f\"Config file not found at {config_path}\")\n",
        "        return None\n",
        "    \n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    return config\n",
        "\n",
        "def save_config(config, suffix=None):\n",
        "    \"\"\"Save a configuration with an optional suffix\"\"\"\n",
        "    if config is None:\n",
        "        print(\"No config to save\")\n",
        "        return None\n",
        "    \n",
        "    if suffix:\n",
        "        config_path = Path(project_root) / \"binary_extractor\" / f\"cfg_{suffix}.yaml\"\n",
        "    else:\n",
        "        config_path = Path(project_root) / \"binary_extractor\" / \"cfg.yaml\"\n",
        "    \n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    print(f\"Config saved to {config_path}\")\n",
        "    return config_path\n",
        "\n",
        "# Load the current configuration\n",
        "current_config = load_config()\n",
        "\n",
        "if current_config:\n",
        "    # Display the current configuration\n",
        "    print(\"Current Configuration:\")\n",
        "    for section, params in current_config.items():\n",
        "        print(f\"\\n{section}:\")\n",
        "        if isinstance(params, dict):\n",
        "            for key, value in params.items():\n",
        "                print(f\"  {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"  {params}\")\n",
        "else:\n",
        "    print(\"Failed to load configuration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the pipeline with a specific configuration\n",
        "def run_pipeline_with_config(config=None, output_dir=None, suffix=None):\n",
        "    \"\"\"Run the pipeline with a specific configuration\"\"\"\n",
        "    if config is None:\n",
        "        print(\"No configuration provided\")\n",
        "        return None\n",
        "    \n",
        "    # Save the configuration if a suffix is provided\n",
        "    if suffix:\n",
        "        config_path = save_config(config, suffix)\n",
        "    else:\n",
        "        config_path = Path(project_root) / \"binary_extractor\" / \"cfg.yaml\"\n",
        "        # Save the current config temporarily\n",
        "        with open(config_path, 'w') as f:\n",
        "            yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    # Set up the output directory\n",
        "    if output_dir is None:\n",
        "        if suffix:\n",
        "            output_dir = Path(project_root) / \"binary_extractor\" / f\"output_{suffix}\"\n",
        "        else:\n",
        "            output_dir = Path(project_root) / \"binary_extractor\" / \"output\"\n",
        "    \n",
        "    # Make sure the output directory exists\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Set up the pipeline\n",
        "    try:\n",
        "        # Create a pipeline instance\n",
        "        pipeline = Pipeline(\n",
        "            config_path=str(config_path),\n",
        "            output_dir=str(output_dir)\n",
        "        )\n",
        "        \n",
        "        # Run the pipeline\n",
        "        print(f\"Running pipeline with configuration{' ' + suffix if suffix else ''}...\")\n",
        "        pipeline.run()\n",
        "        \n",
        "        print(f\"Pipeline completed. Results saved to {output_dir}\")\n",
        "        return output_dir\n",
        "    except Exception as e:\n",
        "        print(f\"Error running pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example function to modify a configuration for tuning\n",
        "def create_tuned_config(base_config, changes, suffix):\n",
        "    \"\"\"Create a tuned configuration by applying changes to a base config\"\"\"\n",
        "    if base_config is None:\n",
        "        print(\"No base configuration provided\")\n",
        "        return None\n",
        "    \n",
        "    # Create a deep copy of the base config\n",
        "    import copy\n",
        "    tuned_config = copy.deepcopy(base_config)\n",
        "    \n",
        "    # Apply changes\n",
        "    for section, params in changes.items():\n",
        "        if section not in tuned_config:\n",
        "            tuned_config[section] = {}\n",
        "        \n",
        "        if isinstance(params, dict):\n",
        "            for key, value in params.items():\n",
        "                tuned_config[section][key] = value\n",
        "        else:\n",
        "            tuned_config[section] = params\n",
        "    \n",
        "    # Save the tuned config\n",
        "    config_path = save_config(tuned_config, suffix)\n",
        "    \n",
        "    return tuned_config\n",
        "\n",
        "# Example: Create a tuned configuration with different threshold values\n",
        "# Uncomment and modify as needed\n",
        "\"\"\"\n",
        "if current_config:\n",
        "    # Create a tuned configuration with lower threshold\n",
        "    tuned_config_1 = create_tuned_config(\n",
        "        current_config,\n",
        "        {\n",
        "            'preprocessing': {\n",
        "                'threshold_value': 120  # Lower threshold value\n",
        "            }\n",
        "        },\n",
        "        'lower_threshold'\n",
        "    )\n",
        "    \n",
        "    # Run the pipeline with the tuned configuration\n",
        "    output_dir_1 = run_pipeline_with_config(tuned_config_1, suffix='lower_threshold')\n",
        "    \n",
        "    # Load and analyze the results\n",
        "    if output_dir_1:\n",
        "        _, pipeline_digits_1, pipeline_overlay_1 = load_pipeline_output(output_dir_1)\n",
        "        analyze_digit_distribution(pipeline_digits_1, \"Lower Threshold - Digit Distribution\")\n",
        "        compare_datasets(pipeline_digits_1, reference_digits, \"Lower Threshold - Comparison\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conclusion and Recommendations\n",
        "\n",
        "Based on the analysis of the pipeline output compared to the reference data, we can make the following observations and recommendations:\n",
        "\n",
        "1. **Distribution of 0s and 1s**: \n",
        "   - The reference data should have a relatively even distribution of 0s and 1s\n",
        "   - If the pipeline output shows a significant skew, consider adjusting the classification parameters\n",
        "\n",
        "2. **Overlay Detection**:\n",
        "   - Accurate overlay detection is crucial for avoiding false positives\n",
        "   - If overlay detection is missing cells, consider adjusting the overlay detection parameters\n",
        "\n",
        "3. **Parameter Tuning Recommendations**:\n",
        "   - **Preprocessing**: Adjust threshold values, blur kernel size, and morphological operations\n",
        "   - **Grid Detection**: Fine-tune grid origin and cell size\n",
        "   - **Classification**: Adjust template matching threshold or classification algorithm\n",
        "\n",
        "4. **Next Steps**:\n",
        "   - Run grid search over key parameters to find optimal configuration\n",
        "   - Validate results against reference data\n",
        "   - Document the optimal configuration and results\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
